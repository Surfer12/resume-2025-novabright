#!/usr/bin/env python3
"""
Basic Meta-Optimization Example

This example demonstrates the basic usage of the Meta-Optimization Framework
for cognitive task optimization. It shows how to:

1. Create cognitive task data
2. Set up the meta-optimizer
3. Run optimization
4. Analyze results

Target Performance:
- 19% ± 8% accuracy improvement (95% CI: [11%, 27%])
- 12% ± 4% computational efficiency gains (95% CI: [8%, 16%])
- 22% ± 5% cognitive load reduction
"""

try:
    import torch
except ImportError as e:
    print("Error: PyTorch is not installed. Please install it using 'pip install torch'.")
    raise e
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import logging

# Import framework components
import sys
sys.path.append('..')
from src.core.meta_optimization import MetaOptimizer, TaskSpecification
from src.utils.data_processing import DataProcessor
from src.utils.statistical_analysis import StatisticalAnalyzer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """Main example function."""
    print("=" * 60)
    print("Meta-Optimization Framework - Basic Example")
    print("=" * 60)

    # Set random seed for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)

    # 1. Create cognitive task data
    print("\n1. Generating cognitive task data...")
    data_processor = DataProcessor(random_seed=42)

    # Generate N-back task data (working memory)
    input_data, target_data = data_processor.generate_n_back_data(
        batch_size=64,
        sequence_length=20,
        n_back=2,
        feature_dim=128,
        match_probability=0.3
    )

    print(f"   Input shape: {input_data.shape}")
    print(f"   Target shape: {target_data.shape}")
    print(f"   Match rate: {torch.mean(target_data):.3f}")

    # 2. Create task specification
    print("\n2. Creating task specification...")
    task_spec = TaskSpecification(
        input_data=input_data,
        target_output=target_data,
        task_type="n_back",
        cognitive_constraints={
            "authenticity": 0.8,        # High cognitive authenticity required
            "memory_capacity": 7,        # Miller's 7±2 rule
            "attention_threshold": 0.7,  # Attention focus requirement
            "expected_biases": 0.3  # Expected confirmation bias
        },
        efficiency_requirements={
            "max_flops": 1e9,           # Maximum floating point operations
            "max_memory": 1e6,          # Maximum memory usage
            "max_complexity": 1.0       # Maximum computational complexity
        }
    )

    print(f"   Task type: {task_spec.task_type}")
    print(f"   Cognitive constraints: {task_spec.cognitive_constraints}")
    print(f"   Efficiency requirements: {task_spec.efficiency_requirements}")

    # 3. Initialize meta-optimizer
    print("\n3. Initializing meta-optimizer...")
    optimizer = MetaOptimizer(
        cognitive_constraints=task_spec.cognitive_constraints,
        efficiency_requirements=task_spec.efficiency_requirements,
        random_seed=42
    )

    print(f"   Optimizer initialized with {len(optimizer.cognitive_regularizer.constraints)} cognitive constraints")
    print(f"   Active bias types: {list(optimizer.bias_modeler.bias_implementations.keys())}")

    # 4. Run meta-optimization
    print("\n4. Running meta-optimization...")
    print("   This may take a few minutes...")

    result = optimizer.meta_optimize(
        task_specification=task_spec,
        max_iterations=100,
        convergence_threshold=1e-6,
        target_improvement=0.19  # Target 19% improvement
    )

    # 5. Analyze results
    print("\n5. Optimization Results:")
    print("=" * 40)
    print(f"   Performance Gain: {result.performance_gain:.1%}")
    print(f"   Confidence Interval: {result.confidence_interval}")
    print(f"   Effect Size: {result.effect_size}")
    print(f"   Convergence Iterations: {len(result.convergence_history)}")
    print(f"   Failures Encountered: {len(result.failure_modes)}")

    print("\n   Final Parameters:")
    print(f"   α (integration): {result.alpha:.3f}")
    print(f"   λ₁ (cognitive): {result.lambda_1:.3f}")
    print(f"   λ₂ (efficiency): {result.lambda_2:.3f}")
    print(f"   β (bias): {result.beta:.3f}")

    # 6. Statistical validation
    print("\n6. Statistical Validation:")
    print("=" * 40)

    analyzer = StatisticalAnalyzer()

    # Validate against target metrics
    if len(result.convergence_history) > 1:
        # Convert performance to percentage improvements
        baseline_performance = 0.5  # Assume 50% baseline
        improvements = [(p - baseline_performance) / baseline_performance * 100
                       for p in result.convergence_history]

        validation_results = analyzer.validate_target_metrics(
            accuracy_improvements=improvements,
            efficiency_gains=[12.0] * len(improvements),  # Placeholder
            cognitive_load_reductions=[22.0] * len(improvements)  # Placeholder
        )

        print(f"   Accuracy target met: {validation_results.get('accuracy_target_met', False)}")
        print(f"   Mean improvement: {validation_results.get('accuracy_mean', 0):.1f}%")

        # Hypothesis test for improvement
        t_test = analyzer.t_test(
            improvements,
            mu=0,
            alternative="greater"
        )
        print(f"   Statistical significance: {t_test}")

    # 7. Visualize results
    print("\n7. Creating visualizations...")
    create_visualizations(optimizer, result)

    # 8. Component analysis
    print("\n8. Component Analysis:")
    print("=" * 40)

    # Dynamic integration analysis
    integration_summary = optimizer.dynamic_integrator.get_integration_summary()
    print(f"   Integration stability: {integration_summary.get('integration_stability', 0):.3f}")
    print(f"   Symbolic dominance: {integration_summary.get('symbolic_dominance', 0):.3f}")

    # Cognitive regularization analysis
    regularization_summary = optimizer.cognitive_regularizer.get_regularization_summary()
    print(f"   Mean authenticity: {regularization_summary.get('mean_authenticity', 0):.3f}")
    print(f"   Authenticity stability: {regularization_summary.get('authenticity_stability', 0):.3f}")

    # Bias modeling analysis
    bias_summary = optimizer.bias_modeler.get_bias_summary()
    print(f"   Active biases: {bias_summary.get('active_biases', 0)}")
    print(f"   Mean bias effect: {bias_summary.get('mean_bias_effect', 0):.3f}")

    # 9. Failure analysis
    if result.failure_modes:
        print("\n9. Failure Analysis:")
        print("=" * 40)

        failure_analysis = optimizer.failure_documenter.analyze_failure_patterns()
        print(f"   Total failures: {failure_analysis.get('total_failures', 0)}")
        print(f"   Most common failure: {failure_analysis.get('most_common_failure', 'None')}")

        # Get recommendations
        recommendations = optimizer.failure_documenter.get_failure_recommendations(
            current_parameters={
                "alpha": result.alpha,
                "lambda_1": result.lambda_1,
                "lambda_2": result.lambda_2,
                "beta": result.beta
            },
            current_context={"task_type": task_spec.task_type}
        )

        print("   Recommendations:")
        for rec in recommendations[:3]:  # Show top 3 recommendations
            print(f"   - {rec}")

    # 10. Summary and next steps
    print("\n10. Summary and Next Steps:")
    print("=" * 40)

    # Check if target performance was achieved
    target_achieved = result.performance_gain >= 0.11  # Conservative target (11%)

    if target_achieved:
        print("   ✓ Target performance improvement achieved!")
        print("   ✓ Framework successfully optimized cognitive task")
        print("   → Ready for production use or further experimentation")
    else:
        print("   ⚠ Target performance not yet achieved")
        print("   → Consider adjusting parameters or increasing iterations")
        print("   → Review failure modes and apply recommendations")

    print(f"\n   Cognitive authenticity: {'High' if regularization_summary.get('mean_authenticity', 0) > 0.7 else 'Moderate'}")
    print(f"   System stability: {'High' if integration_summary.get('integration_stability', 0) > 0.8 else 'Moderate'}")
    print(f"   Bias modeling: {'Active' if bias_summary.get('active_biases', 0) > 0 else 'Inactive'}")

    print("\n" + "=" * 60)
    print("Example completed successfully!")
    print("Check the 'results/' directory for detailed outputs and visualizations.")
    print("=" * 60)

def create_visualizations(optimizer: MetaOptimizer, result) -> None:
    """Create visualizations of optimization results."""

    # Create results directory
    results_dir = Path("results")
    results_dir.mkdir(exist_ok=True)

    # 1. Convergence plot
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.plot(result.convergence_history)
    plt.title("Performance Convergence")
    plt.xlabel("Iteration")
    plt.ylabel("Performance")
    plt.grid(True)

    # 2. Parameter evolution
    plt.subplot(2, 2, 2)
    if optimizer.alpha_history:
        plt.plot(optimizer.alpha_history, label="α (integration)")
    if optimizer.lambda_history:
        lambda1_history = [l[0] for l in optimizer.lambda_history]
        lambda2_history = [l[1] for l in optimizer.lambda_history]
        plt.plot(lambda1_history, label="λ₁ (cognitive)")
        plt.plot(lambda2_history, label="λ₂ (efficiency)")
    if optimizer.beta_history:
        plt.plot(optimizer.beta_history, label="β (bias)")

    plt.title("Parameter Evolution")
    plt.xlabel("Iteration")
    plt.ylabel("Parameter Value")
    plt.legend()
    plt.grid(True)

    # 3. Performance distribution
    plt.subplot(2, 2, 3)
    plt.hist(result.convergence_history, bins=20, alpha=0.7, edgecolor='black')
    plt.axvline(result.performance_gain, color='red', linestyle='--',
                label=f'Final: {result.performance_gain:.3f}')
    plt.title("Performance Distribution")
    plt.xlabel("Performance")
    plt.ylabel("Frequency")
    plt.legend()
    plt.grid(True)

    # 4. Component contributions
    plt.subplot(2, 2, 4)
    if hasattr(optimizer.dynamic_integrator, 'symbolic_contributions') and \
       optimizer.dynamic_integrator.symbolic_contributions:

        symbolic_contrib = optimizer.dynamic_integrator.symbolic_contributions
        neural_contrib = optimizer.dynamic_integrator.neural_contributions

        plt.plot(symbolic_contrib, label="Symbolic", alpha=0.7)
        plt.plot(neural_contrib, label="Neural", alpha=0.7)
        plt.title("Component Contributions")
        plt.xlabel("Iteration")
        plt.ylabel("Contribution Magnitude")
        plt.legend()
        plt.grid(True)
    else:
        plt.text(0.5, 0.5, "No component data available",
                ha='center', va='center', transform=plt.gca().transAxes)
        plt.title("Component Contributions")

    plt.tight_layout()
    plt.savefig(results_dir / "optimization_results.png", dpi=300, bbox_inches='tight')
    plt.close()

    print(f"   Visualizations saved to {results_dir}/optimization_results.png")

if __name__ == "__main__":
    main()
